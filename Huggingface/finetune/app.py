import time
from typing import Dict, List

import spacy
from transformers import AutoTokenizer

from BertBilstmCRF import BertBilstmCrf

# 用于测试
names = [
    "O",
    "B-DNAMutation",
    "I-DNAMutation",
    "B-ProteinMutation",
    "I-ProteinMutation",
    "B-SNP",
    "I-SNP"
]


def process_token(token):
    if token[:2] == '##':
        return token[2:]
    return token


def token2word(token1, token2):
    return token1 + token2


if __name__ == '__main__':
    #
    t_init = time.time()
    tokenizer = AutoTokenizer.from_pretrained('checkpoint', )
    # tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-base-cased-v1.1",
    #                                           local_files_only=True,
    #                                           do_lower_case=False  ## Note 大小写敏感
    #                                           )
    model = BertBilstmCrf.from_pretrained('checkpoint')
    # model = torch.load('checkpoint_pt/')
    preprocessos = spacy.load('en_core_web_sm')

    document = """Catalytic cysteine of thymidylate synthase is activated upon substrate binding.The role of Ser 167 of Escherichia coli thymidylate synthase (TS) in catalysis has been characterized by kinetic and crystallographic studies. Position 167 variants including S167A, S167N, S167D, S167C, S167G, S167L, S167T, and S167V were generated by site-directed mutagenesis. Only S167A, S167G, S167T, and S167C complemented the growth of thymidine auxotrophs of E. coli in medium lacking thymidine. Steady-state kinetic analysis revealed that mutant enzymes exhibited k(cat) values 1.1-95-fold lower than that of the wild-type enzyme. Relative to wild-type TS, K(m) values of the mutant enzymes for 2'-deoxyuridylate (dUMP) were 5-90 times higher, while K(m) values for 5,10-methylenetetrahydrofolate (CH(2)H(4)folate) were 1.5-16-fold higher. The rate of dehalogenation of 5-bromo-2'-deoxyuridine 5'-monophosphate (BrdUMP), a reaction catalyzed by TS that does not require CH(2)H(4)folate as cosubstrate, by mutant TSs was analyzed and showed that only S167A and S167G catalyzed the dehalogenation reaction and values of k(cat)/K(m) for the mutant enzymes were decreased by 10- and 3000-fold, respectively. Analysis of pre-steady-state kinetics of ternary complex formation revealed that the productive binding of CH(2)H(4)folate is weaker to mutant TSs than to the wild-type enzyme. Chemical transformation constants (k(chem)) for the mutant enzymes were lower by 1.1-6.0-fold relative to the wild-type enzyme. S167A, S167T, and S167C crystallized in the I2(1)3 space group and scattered X-rays to either 1.7 A (S167A and S167T) or 2.6 A (S167C). The high-resolution data sets were refined to a R(crys) of 19.9%. In the crystals some cysteine residues were derivatized with 2-mercaptoethanol to form S,S-(2-hydroxyethyl)thiocysteine. The pattern of derivatization indicates that in the absence of bound substrate the catalytic cysteine is not more reactive than other cysteines. It is proposed that the catalytic cysteine is activated by substrate binding by a proton-transfer mechanism in which the phosphate group of the nucleotide neutralizes the charge of Arg 126', facilitating the transfer of a proton from the catalytic cysteine to a His 207-Asp 205 diad via a system of ordered water molecules."""

    doc = preprocessos(document)
    sentences = [sent.text.split(' ') for sent in doc.sents]

    pre_seq = None

    # tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))

    t0 = time.time()
    # inputs = tokenizer(pre_seq, return_tensors="pt", is_split_into_words=True)
    inputs = tokenizer(sentences, return_tensors="pt", padding=True, truncation=True, is_split_into_words=True)
    word_ids = inputs.word_ids()
    tokens = inputs.tokens()
    # for t in inputs.values():
    #     t = t.to('cuda')

    predictions = model.predict(**inputs)

    t1 = time.time()

    ## Note 根据word聚集
    words_group = []
    word_spans: Dict[int, list] = {}
    for idx, prediction in enumerate(predictions):
        for word_idx, token, label_id in zip(inputs.word_ids(idx), inputs.tokens(idx), prediction):

            # for word_idx, token, prediction in zip(word_ids, tokens, predictions[0]):
            #         # print((token, model.config.id2label[prediction]))
            if word_idx is not None:
                word_spans.setdefault(word_idx, {
                    'tokens': [],
                    'labels': []
                })
                word_spans[word_idx]['tokens'].append(process_token(token))
                word_spans[word_idx]['labels'].append(names[label_id])
        words_group.append(word_spans)
        word_spans = {}

    ## Note 根据label聚集 只筛选实体所在的token
    spans: List[Dict[str, List[str]]] = []
    group = {}
    for idx, prediction in enumerate(predictions):
        for word_idx, token, label_id in zip(inputs.word_ids(idx), inputs.tokens(idx), prediction):

            # print(f'{token}\t{names[label_id]}')
            ## Bug 连续实体问题
            if names[label_id] == 'O':
                if group:
                    spans.append(group)
                    group = {}  ###
            else:
                if group:
                    # 处理连续实体问题.
                    if label_id in [1, 3, 5]:
                        spans.append(group)
                        group = {}
                    elif names[label_id][2:] != group['labels'][-1][2:]:
                        spans.append(group)
                        group = {}

                group.setdefault('tokens', [])
                group.setdefault('word_ids', [])
                group.setdefault('labels', [])
                group['tokens'].append(process_token(token))
                group['word_ids'].append(word_idx)
                group['labels'].append(names[label_id])


    def entity_gen(tokens, word_ids):
        previous_idx = None
        entity_name = ''
        for token, idx in zip(tokens, word_ids):
            if idx != previous_idx and entity_name:
                entity_name += ' ' + token
            else:
                entity_name += token
            previous_idx = idx
        return entity_name


    entitys: List[Dict[str, str]] = []
    for span in spans:
        entity_name = entity_gen(span['tokens'], span['word_ids'])
        entity_label = span['labels'][-1][2:]
        entitys.append({
            'name': entity_name,
            'label': entity_label
        })
    t2 = time.time()
for ent in entitys:
    print(ent['name'])
